{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Learning Goals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Able to install and setup Selenium\n",
    "- Able to login to website platform\n",
    "- Able to navigate through pages/pop-ups\n",
    "- Able to write scraper that is more \"human-like\"\n",
    "- Able to know when to use appropriate `find_element(s)_by...`\n",
    "- Able to acquire desired data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, head over to [this page](https://chromedriver.chromium.org/downloads) and locate the chromedriver that matches your chrome version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to Find Your Internet Browser Version Number - Google Chrome.**\n",
    "\n",
    "1) Click on the Menu icon in the upper right corner of the screen. \n",
    "\n",
    "2) Click on Help, and then About Google Chrome. \n",
    "\n",
    "3) Your Chrome browser version number can be found here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next, download the appropriate driver that matches your version of Chrome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After you have downloaded the driver, press `command` + `spacebar`\n",
    "- Inside of the spotlight search you just opened, type `/usr/local/bin/` and open that folder\n",
    "- Next, in a separate finder window (`command` + `n`), navigate to where you downloaded the `chromedriver`\n",
    "- Finally, move the `chromedriver` from where ever you downloaded it into your `/usr/local/bin/`\n",
    "\n",
    "*Technically, you can install the driver anywhere, but most tutorials I have read say to put it in `/usr/local/bin/`*\n",
    "\n",
    "...However, after a bit of research, I believe the reason we want to install the `chromedriver` inside of `/usr/local/bin/` is so that you don't have to explicitly state the chromedriver path when you instantiate your driver ðŸ˜Ž \n",
    "\n",
    "https://www.kenst.com/2015/03/including-the-chromedriver-location-in-macos-system-path/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Selenium if you have not already done so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please complete the above steps before lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import system   \n",
    "from math import floor\n",
    "from copy import deepcopy\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next, I always like to label my driver with a bold title cell**\n",
    "\n",
    "\n",
    "- I find it helps when we need to re-instantiate our driver and for general organization\n",
    "- Also, when we copy and paste this code form a notebook to a .py file, we would usually only need one driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DRIVER HERE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver = webdriver.Chrome()\n",
    "\n",
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Chrome('/Users/meaganrossi/Projects/Scraping_Tools/chromedriver')  # Optional argument, if not specified will search path.\n",
    "driver.get('http://www.google.com/');\n",
    "time.sleep(5) # Let the user actually see something!\n",
    "search_box = driver.find_element_by_name('q')\n",
    "search_box.send_keys('ChromeDriver')\n",
    "search_box.submit()\n",
    "time.sleep(5) # Let the user actually see something!\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Headless Browsers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Headless Browser**\n",
    "A Headless Browser is also a Web Browser but without a graphical user interface (GUI) but can be controlled programmatically which can be extensively used for automation, testing, and other purposes.\n",
    "\n",
    "**Why to use Headless Browsers?**\n",
    "There are a lot of advantages and disadvantages in using the Headless Browsers. Using a headless browser might not be very helpful for browsing the Web, but for Automating tasks and tests itâ€™s awesome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages of Headless Browsers**\n",
    "\n",
    "Some of the advantages are as follows:\n",
    "\n",
    "- Headless Browsers are typically faster than real browsers. \n",
    "    - The reason for being faster is because we are not starting up a Browser GUI and can bypass all the time a real browser takes to load CSS, JavaScript and open and render HTML DOM.\n",
    "- Performance wise, you can typically see a 2x to 15x faster performance when using a headless browser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*More info on headless browsers here:* https://stackoverflow.com/questions/53083952/difference-of-headless-browsers-for-automation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to scrape!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://media1.tenor.com/images/3fd84ba4b54f8d299f7732e63cdb3c00/tenor.gif?itemid=11903546\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visiting a webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit the website of your choice:\n",
    "\n",
    "driver.get('https://www.espn.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods for finding a single element \n",
    "\n",
    "    This will return the FIRST instance of your desired \"element\"\n",
    "\n",
    "* find_element_by_id\n",
    "* find_element_by_name\n",
    "* find_element_by_xpath  \n",
    "* find_element_by_link_text\n",
    "* find_element_by_partial_link_text\n",
    "* find_element_by_tag_name\n",
    "* find_element_by_class_name\n",
    "* find_element_by_css_selector\n",
    "\n",
    "---\n",
    "\n",
    "#### Methods for finding multiple elements\n",
    "\n",
    "    This will return a list of ALL instances of your desired \"element\"\n",
    "\n",
    "* find_elements_by_name\n",
    "* find_elements_by_xpath\n",
    "* find_elements_by_link_text\n",
    "* find_elements_by_partial_link_text\n",
    "* find_elements_by_tag_name\n",
    "* find_elements_by_class_name\n",
    "* find_elements_by_css_selector\n",
    "\n",
    "From the [Selenium Python Docs](https://selenium-python.readthedocs.io/locating-elements.html \"Selenium Docs\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the FIRST instance of an \"element\"\n",
    "\n",
    "First, well check out `.find_element_by_css_selector()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_css_selector('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_tag_name('h1').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting ALL instances of your desired \"element\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'driver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6b897c4fbc23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlisty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_elements_by_css_selector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'h1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'driver' is not defined"
     ]
    }
   ],
   "source": [
    "listy = driver.find_elements_by_css_selector('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in listy[:15]:\n",
    "    if len(x.text) > 0:\n",
    "        print(x.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting a specific element (by class name)\n",
    "\n",
    "Using `.find_element_by_class_name()` to locate an element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_class_name('contentItem__title--hero').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Closing the driver:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you were to just close your driver's browsing window, your Google chrome instance will still appear open in your mac's dock. Using `driver.quit()`, we can close the Google chrome instance, which will also close the driver's browser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging into websites\n",
    "\n",
    "We'll use `.find_element_by_id()` for this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from private import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_url = 'https://www.facebook.com'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(my_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = driver.find_element_by_id(\"email\")\n",
    "password = driver.find_element_by_id(\"pass\")\n",
    "submit   = driver.find_element_by_id(\"loginbutton\")\n",
    "  \n",
    "username.send_keys(FB_USERNAME)\n",
    "password.send_keys(PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we will need to wait for the page to load. Other times, we may want to have our scraper act more like a human, in terms of \"click rate.\"\n",
    "\n",
    "Two possible ways to make this happen are by using `time.sleep()` or `WebDriverWait()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we just want to mimic the behavior of a human, we can use `time.sleep()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a single \"wait\" time:\n",
    "\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a randomized time:\n",
    "\n",
    "sequence = [x/10 for x in range(8, 14)]\n",
    "print(sequence)\n",
    "\n",
    "time.sleep(random.choice(sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we explicitly want to wait for our page to load, we can use `WebDriverWait()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait = WebDriverWait(driver, 5)\n",
    "\n",
    "try:\n",
    "    page_loaded = wait.until(lambda driver: driver.current_url == my_url)\n",
    "    print('The page loaded correctly')\n",
    "except TimeoutException:\n",
    "    print(\"Loading timeout expired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.current_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ohhhh nooooooo, I can't remember how I named my variables...\n",
    "\n",
    "And I don't want to open the file elsewhere to check, because that seems inefficient..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(locals().keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous output is a bit messy... \n",
    "\n",
    "If we are writing a .py file specifically to store \"private\" variables, I recommend using an all caps syntax. The two reasons I like this are:\n",
    "\n",
    "    1) This mimics the syntax of ENVIRONMENT_VARIABLES\n",
    "\n",
    "    2) If we name our private.py file variables with all caps, we can see all our private variable names like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(globals().keys()):\n",
    "    if key[-1] == key[-1].title() and key[-1].isalpha() == True:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The variable \"`EC`\" is present in the list above because of how we imported the `expected_conditions` module up at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.instagram.com/')\n",
    "\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### The old IG landing page used this, but they have recently updated their landing page ####\n",
    "#### Just leaving for notes ####\n",
    "\n",
    "# Find the login click button\n",
    "# ig_login_button = driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/article/div[2]/div[2]/p/a')\n",
    "\n",
    "# Click the button\n",
    "# ig_login_button.click()\n",
    "\n",
    "# time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wait a second... what is that `xpath` thing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XPath is defined as XML path. It is a syntax or language for finding any element on the web page using XML path expression. XPath is used to find the location of any element on a webpage using HTML DOM structure. The basic format of XPath is explained below with screen shot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://www.guru99.com/images/3-2016/032816_0758_XPathinSele1.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XPath contains the path of the element situated at the web page. Standard syntax for creating XPath is:\n",
    "\n",
    "`Xpath=//tagname[@attribute='value']`\n",
    "\n",
    "- // == Select current node.\n",
    "- Tagname == Tagname of the particular node.\n",
    "- @ == Select attribute.\n",
    "- Attribute == Attribute name of the node.\n",
    "- Value == Value of the attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://media1.giphy.com/media/XBpEStoQ5rftPFA8rh/giphy.gif?cid=790b7611dbcd651cd785fb8382888f7b41666d5c8695755b&rid=giphy.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can perform the next operations a few different ways:**\n",
    "\n",
    "Similar to above, we could use the `xpath`\n",
    "\n",
    "Or... based on visual knowledge of inspecting html/css elements, we can see the css selector `input` and we could assume that the only 2 possible inputs are Username and Password\n",
    "\n",
    "---\n",
    "\n",
    "With that knowledge, we can define both variables in one line of code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_username, ig_password = driver.find_elements_by_css_selector('input')\n",
    "# driver.find_elements_by_css_selector('input')\n",
    "\n",
    "# ig_username = driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[2]/div/label/input')\n",
    "# ig_password = driver.find_element_by_xpath('//*[@id=\"react-root\"]/section/main/div/article/div/div[1]/div/form/div[3]/div/label/input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_username.send_keys(INSTA_USERNAME)\n",
    "ig_password.send_keys(PASSWORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is the complete xpath to the login element:\n",
    "\n",
    "login_button_xpath = '//*[@id=\"react-root\"]/section/main/article/div[2]/div[1]/div/form/div[4]/button'\n",
    "\n",
    "ig_submit = driver.find_element_by_xpath(login_button_xpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_submit.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sometimes, depending on the HTML layout, we might want to truncate the xpath:\n",
    "\n",
    "# ig_submit = driver.find_element_by_xpath('//div[4]/button/div')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modal buttons and scrolling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whoah! What's that modal? \n",
    "try:\n",
    "    modal_button = driver.find_element_by_class_name(\"HoLwm\")\n",
    "    modal_button.click()\n",
    "    \n",
    "except: \n",
    "    pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These websites have modal popups:\n",
    "\n",
    "driver.get('https://www.nike.com')\n",
    "\n",
    "# Other options:\n",
    "# https://www.carbon38.com\n",
    "# https://www.meundies.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is an example of how you can write functions to scroll down the page (for dynamic loading) and for loading more content with \"clicks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Scroll down (with a test for a modal)\n",
    "\n",
    "def scroll_down():\n",
    "    for i in range(1, 10):\n",
    "        try:\n",
    "            modal_button = driver.find_element_by_class_name(\"button2\")\n",
    "            webdriver.ActionChains(driver).move_to_element(modal_button).click(modal_button).perform()\n",
    "      ##### modal_button.click() also works \n",
    "            \n",
    "        except:\n",
    "            time.sleep(.5)\n",
    "            pass \n",
    "        \n",
    "        #scroll to the bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1)\n",
    "\n",
    "        \n",
    "# Example: Load more content\n",
    "# Code snippet for context purposes only. We will not run this function:\n",
    "\n",
    "def get_more(): \n",
    "    for i in range(1, 5):\n",
    "        try:\n",
    "            next_b = driver.find_element_by_xpath(\"//*[contains(text(), 'Load next Politics story')]\")\n",
    "            webdriver.ActionChains(driver).move_to_element(next_b).click(next_b).perform()\n",
    "            time.sleep(.5)\n",
    "        except: \n",
    "            print(\"Page #\" + str(i) + \" has failed to load\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell and watch the page scrollllllll\n",
    "\n",
    "scroll_down()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use BeautifulSoup vs.  Selenium?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://media.giphy.com/media/xTiN0IuPQxRqzxodZm/giphy.gif' width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://media2.giphy.com/media/3o7TKAdOad9Y3eSMZG/giphy.gif?cid=790b761168b43f2be748800602251dce3cad91fcb4c972f9&rid=giphy.gif' width = 400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://media1.giphy.com/media/8VLgtJqaxIlhu/giphy.gif?cid=790b7611df175494e219b99894f7e717b3ea7bfbf806f9c4&rid=giphy.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Just kidding!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything depends on the website and your data goals.\n",
    "\n",
    "In general:\n",
    "- If the data needs to be exposed interactively, then go for Selenium. \n",
    "- Selenium for more complex JavaScript heavy pages. \n",
    "---\n",
    "- If the data is accessible in the HTML structure (more static pages), soup is a more lightweight tool. \n",
    "- Soup gives you more control about navigating the HTML tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get('https://www.skysports.com/premier-league-table')\n",
    "bs = BeautifulSoup(html.content, 'lxml')\n",
    "table = bs.table\n",
    "\n",
    "# table = bs.find(lambda tag: tag.name=='table' ) \n",
    "# rows = table.findAll(lambda tag: tag.name=='tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_rows = table.find_all('tr')\n",
    "\n",
    "for tr in table_rows:\n",
    "    td = tr.find_all('td')\n",
    "    row = [i.text for i in td]\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get('https://www.skysports.com/premier-league-table')\n",
    "bs = BeautifulSoup(html.content, 'lxml')\n",
    "table = bs.table\n",
    "\n",
    "#If you know there is more than one table, you can edit the code to include the proper index:\n",
    "# table = bs.find_all('table')[0] \n",
    "\n",
    "df = pd.read_html(str(table), index_col='Team')\n",
    "df = df[0].dropna(axis=0, thresh=4)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting the header and index:\n",
    "\n",
    "- Caveat: this uses pandas, not Selenium or Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is more than one table, pandas reads the html as a list of tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_html('https://www.sportsmole.co.uk/football/premier-league/2018-19/')\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out one of our tables:\n",
    "\n",
    "df2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the table's formatting is slightly off...\n",
    "\n",
    "So we can make adjustments like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_html('https://www.sportsmole.co.uk/football/premier-league/2018-19/',header=0, index_col=1)\n",
    "\n",
    "df2[0].columns =  ['final_standings', 'P', 'W', 'D', 'L', 'F', 'A', 'GD', 'PTS']\n",
    "\n",
    "df2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example where formatting is an issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get('http://www.nfl.com/stats/team')\n",
    "nfl_soup = BeautifulSoup(html.content, 'lxml')\n",
    "table = nfl_soup.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.prettify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfl = pd.read_html('http://www.nfl.com/stats/team')\n",
    "\n",
    "nfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRO-TIP: if you want to instantiate a new df variable from a previous df or list of dfs, \n",
    "# making a copy of the df will save you from a headache\n",
    "\n",
    "offense = deepcopy(nfl[0])\n",
    "offense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offense['Total Offense (YPG)'] = offense['Total Offense (YPG).1']\n",
    "offense.drop(columns=['Total Offense (YPG).1'], inplace=True)\n",
    "offense.columns = ['TEAM', 'Total_Offense_YPG']\n",
    "offense.TEAM = offense.TEAM.str[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Need to refactor this code because the website changed: \n",
    "\n",
    "def clean_data(data_list):\n",
    "    pass\n",
    "#     cleaned = []\n",
    "#     data_copy = deepcopy(data_list)\n",
    "#     for data in data_copy:\n",
    "#         col_1, col_3 = data.columns[0], data.columns[-1]\n",
    "#         cell1 = data.iloc[0,1]\n",
    "#         cell2 = data.iloc[0,2]\n",
    "#         data.iloc[0,0] = cell1\n",
    "#         data.iloc[0,1] = cell2\n",
    "#         data.drop([col_3],axis=1,inplace=True)\n",
    "#         data.columns = ['team', col_1]\n",
    "#         data['team'] = data['team'].apply(lambda x: x.split('.\\xa0')[1] if '.\\xa0' in x else x.split('. ')[1])\n",
    "#         data.total_offense_ypg = data[col_1].astype(float).astype(int)\n",
    "#         cleaned.append(data)\n",
    "#     return cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't run these cells until `clean_data()` has been refactored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_list = deepcopy(nfl[:6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tables = clean_data(tables_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tables[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best example of when Selenium is supreme:\n",
    "\n",
    "When the page is written in JavaScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get('http://www.tennisabstract.com/cgi-bin/player.cgi?p=RogerFederer')\n",
    "bs = BeautifulSoup(html.content, 'lxml')\n",
    "table = bs.table\n",
    "\n",
    "# table = bs.find(lambda tag: tag.name=='table' ) \n",
    "# rows = table.findAll(lambda tag: tag.name=='tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'webdriver' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bfb4cd3167e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://www.tennisabstract.com/cgi-bin/player.cgi?p=RogerFederer\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChrome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'webdriver' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"http://www.tennisabstract.com/cgi-bin/player.cgi?p=RogerFederer\"\n",
    "driver = webdriver.Chrome('/Users/meaganrossi/Projects/Incarceration_COVID/chromedriver')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = driver.find_element_by_id(\"recent-results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body = table.find_element_by_css_selector('tbody')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Table rows usually have the css tag 'tr'\n",
    "rows = body.find_elements_by_css_selector('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows[0].get_attribute('innerHTML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_data = rows[0].find_elements_by_css_selector('td')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in row_data: \n",
    "    print(e.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for r in rows: \n",
    "    row_list = []\n",
    "    row_data = r.find_elements_by_css_selector('td')\n",
    "    for d in row_data: \n",
    "        row_list.append(d.text)\n",
    "    data_list.append(row_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_list[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = table.find_element_by_css_selector('thead')\n",
    "headers.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = headers.text.split(' ')\n",
    "print(columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of columns:     '+ str(len(columns)))\n",
    "print()\n",
    "print('Number of data points: '+ str(len(data_list[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Date','Tournament','Surface','Rd','Rk','vRk', \n",
    "           'Opponent','Score','DR','A%','DF%','1stIn',\n",
    "           '1st%','2nd%','BPSvd','Time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "federer_h2h = pd.DataFrame(data_list[1:], columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "federer_h2h.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A slightly different approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "header = table.find_element_by_css_selector('thead')\n",
    "\n",
    "header_elements = header.find_elements_by_css_selector('th')\n",
    "\n",
    "len(header_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = []\n",
    "\n",
    "for x in header_elements: \n",
    "    headers.append(x.text)\n",
    "print(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some other neat stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a screenshot! \n",
    "\n",
    "driver.get('https://www.nytimes.com')\n",
    "\n",
    "driver.get_screenshot_as_file('ny_times_front_pg.png')\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The .get_attribute() method is your friend\n",
    "# Example code (don't run this):\n",
    "\n",
    "element.get_attribute(\"attribute name\")\n",
    "\n",
    "attribute_value = WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.ID,\n",
    "                                                                \"id_name_here\"))).get_attribute(\"attribute_name_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of \"complete\" scraper:\n",
    "\n",
    "- Including an example of `.get_attribute()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_url_dict = {'15': 'arts', '16':'sports', '24':'sci-tech', '14': 'business',\n",
    "                  '17': 'international', '13': 'authority'}\n",
    "\n",
    "topic_codes = list(topic_url_dict.keys())\n",
    "\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before creating a scraper, examine user interface structure:\n",
    "# Can we scroll through articles? Do we need to get the article links first?\n",
    "\n",
    "driver.get('http://www.satirewire.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-- STEP 1 -->\n",
    "# Create a function to scrape the links of articles:\n",
    "\n",
    "def scrape_links_satirewire(topic_codes):\n",
    "    \n",
    "    base_url = \"http://www.satirewire.com/content1/?cat=\"\n",
    "    link_list = []\n",
    "    \n",
    "    for code in topic_codes: \n",
    "        url = base_url + code \n",
    "        topic = topic_url_dict[code]\n",
    "        driver.get(url)\n",
    "        time.sleep(1.18)\n",
    "        last_page = driver.find_element_by_class_name('pages').text\n",
    "        last_page_value = int(last_page.split(' of ', 1)[1])\n",
    "        link_objects1 = driver.find_elements_by_class_name('morelink')\n",
    "        print('Scraping ', len(link_objects1), topic.upper(), ' article links')\n",
    "        for link in link_objects1:\n",
    "            if '#' not in link.get_attribute('href'):\n",
    "                link_list.append((link.get_attribute('href'), topic))\n",
    "            else:\n",
    "                pass       \n",
    "        for x in range(2, (last_page_value + 1)):\n",
    "            driver.get(url + '&paged=' + str(x))\n",
    "            time.sleep(1.08)\n",
    "            link_objects1 = driver.find_elements_by_class_name('morelink')\n",
    "            print('Scraping ', len(link_objects1), topic.upper(), ' article links')\n",
    "\n",
    "            for link in link_objects1:\n",
    "                if '#' not in link.get_attribute('href'):\n",
    "                    link_list.append((link.get_attribute('href'), topic))\n",
    "                else:\n",
    "                    pass                        \n",
    "    df = pd.DataFrame()\n",
    "    df['urls'] = [x[0] for x in link_list]\n",
    "    df['topics'] = [x[1] for x in link_list]\n",
    "    df = df.drop_duplicates(subset='urls')\n",
    "    set_satirewire_urls = [(df['urls'][i], df['topics'][i]) for i in list(df.index)]\n",
    "    \n",
    "    print('-----------------------------------')\n",
    "    print('Total of', len(set_satirewire_urls), 'urls to scrape for articles')\n",
    "    print('-----------------------------------')\n",
    "    return set_satirewire_urls                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-- STEP 2 -->\n",
    "# Create a helper function to clean up the article's text:\n",
    "\n",
    "def clean_up_satirewire(dirty_string):\n",
    "    \n",
    "    body_clean1 = re.sub(r\"\\s+\", \" \", dirty_string)\n",
    "    body_squeaky = body_clean1.split('Copyright Â©', 1)[0]\n",
    "    sep1 = '(SatireWire) â€” '\n",
    "    sep2 = '(SatireWire.com) â€” '\n",
    "    sep3 = '(SatireWire.com) â€“ '\n",
    "    if sep1 in body_squeaky:\n",
    "        clean = body_squeaky.split(sep1, 1)[1]\n",
    "    elif sep2 in body_squeaky:\n",
    "        clean = body_squeaky.split(sep2, 1)[1]\n",
    "    elif sep3 in body_squeaky:\n",
    "        clean = body_squeaky.split(sep3, 1)[1]\n",
    "    else:\n",
    "        clean = body_squeaky\n",
    "    return clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Are these to \"dash\" characters equal to each other? Answer: ' + str(bool('â€”'=='â€“')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-- STEP 3 -->\n",
    "# Create a helper function to scrape individual article content:\n",
    "\n",
    "def scrape_one_article(url, topic, ind, all_urls, all_dates, all_titles, \n",
    "                       all_lengths, all_topics1, body_contents, source_id):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(1.1)\n",
    "        body = driver.find_element_by_class_name('entry').text\n",
    "        length = round(len(body) /5/ 250, 1)\n",
    "        \n",
    "        if length >= .5:\n",
    "            if url not in all_urls:                \n",
    "                date = driver.find_element_by_class_name('entry-date').text\n",
    "                date = pd.to_datetime(date).date().strftime('%Y-%m-%d')\n",
    "                title = driver.find_element_by_tag_name('h2').text\n",
    "                body_squeaky = clean_up_satirewire(body)\n",
    "                \n",
    "                all_urls.append(url)\n",
    "                body_contents.append(body_squeaky)\n",
    "                all_dates.append(date)\n",
    "                all_titles.append(title)\n",
    "                all_lengths.append(length)\n",
    "                all_topics1.append(topic)\n",
    "\n",
    "#           --- ADDING CATEGORIES AT A LATER TIME FOR TOPIC MODELING ---\n",
    "#                 category_dict = find_categories(content, categories)\n",
    "#                 all_topics1.append(category_dict[0])\n",
    "#                 all_topics2.append(category_dict[1])\n",
    "#                 all_topics3.append(category_dict[2])\n",
    "#                 all_topics4.append(category_dict[3])\n",
    "#                 all_topics5.append(category_dict[4])\n",
    "\n",
    "            else:\n",
    "                print(\"Duplicate link not added\", ind)\n",
    "                pass\n",
    "        else:\n",
    "            print('Not worthy of scraping article #', ind)\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        print('Nothing to scrape for link #', str(ind) , e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <-- STEP 4 -->\n",
    "# Scrape each article's content and populate a dataframe\n",
    "\n",
    "def scrape_satirewire_articles(urls_list):\n",
    "    ind = 1\n",
    "    body_contents = []\n",
    "    all_urls = []\n",
    "    all_dates = []\n",
    "    all_titles = []\n",
    "    all_lengths = []\n",
    "    all_topics1 = []\n",
    "    author = 'Author not specified'\n",
    "    source_id = 'SatireWire'\n",
    "    \n",
    "#     all_topics2 = []\n",
    "#     all_topics3 = []\n",
    "#     all_topics4 = []\n",
    "#     all_topics5 = []\n",
    "    \n",
    "    for url, topic in urls_list:\n",
    "        print('Working on #' + str(ind) + ' of '+ str(len(urls_list)) +' links')\n",
    "        print()\n",
    "        scrape_one_article(url, topic, ind, all_urls, all_dates, all_titles, \n",
    "                           all_lengths, all_topics1, body_contents, source_id)\n",
    "        ind += 1    \n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['body_content'] = body_contents\n",
    "    df['url'] = all_urls\n",
    "    df['date'] = all_dates\n",
    "    df['title'] = all_titles\n",
    "    df['length'] = all_lengths\n",
    "    df['topic_1'] = all_topics1\n",
    "    df['author'] = author\n",
    "    df['source_id'] = source_id\n",
    "    df['satire_or_not'] = 'satire'\n",
    "    df['label'] = 1\n",
    "\n",
    "# ADDING CATEGORIES AT A LATER TIME FOR TOPIC MODELING    \n",
    "#     df['topic_1'] = all_topics1\n",
    "#     df['topic_2'] = all_topics2\n",
    "#     df['topic_3'] = all_topics3\n",
    "#     df['topic_4'] = all_topics4\n",
    "#     df['topic_5'] = all_topics5\n",
    "\n",
    "    df = df.drop_duplicates()\n",
    "    df.index = range(len(df.index))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Complete scraping function\n",
    "\n",
    "def scrape_satirewire():\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    satirewire_urls = scrape_links_satirewire(['16'])    # <--- FOR TESTING/DEMONSTRATION PURPOSES\n",
    "#     satirewore_urls = scrape_links_satirewire(topic_codes)\n",
    "\n",
    "    satirewire_df = scrape_satirewire_articles(satirewire_urls)\n",
    "    print('The satire scraper took ', str(time.time() - start), 'seconds.')  # <---   Can remove before uploading to AWS\n",
    "\n",
    "    return satirewire_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "satire = scrape_satirewire()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "satire.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "satire.body_content[30]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
